{"cells":[{"cell_type":"markdown","metadata":{"id":"9STXm7BIB03T"},"source":["# Fundamentals of Deep Learning -  Playground"]},{"cell_type":"markdown","metadata":{"id":"Wox_KPPeB03U"},"source":["üß† Welcome to this module of Deep Learning!\n","\n","üéØ In this challenge, our goal is two-fold:\n","1. Get a visual representation of Neural Networks\n","2. Build a better intuition of what Neural Networks are doing\n","\n","üëâ We will use ***[Tensorflow Playground](https://playground.tensorflow.org/)***\n","\n","_(This first challenge does not require much coding_)"]},{"cell_type":"markdown","metadata":{"id":"UrM07FQLB03V"},"source":["## Classification in Deep Learning"]},{"cell_type":"markdown","metadata":{"id":"--LqYz5pB03V"},"source":["### (1) The data"]},{"cell_type":"markdown","metadata":{"id":"1ngPftIFB03V"},"source":["‚ùì Let's go to the [Playground](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=spiral&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=2&seed=0.23545&showTestData=false&discretize=false&percTrainData=70&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&regularization_hide=true&showTestData_hide=false&stepButton_hide=false&activation_hide=false&problem_hide=false&batchSize_hide=true&dataset_hide=false&resetButton_hide=false&discretize_hide=false&playButton_hide=false&learningRate_hide=true&regularizationRate_hide=true&percTrainData_hide=false&numHiddenLayers_hide=false) and select the following type of data ‚ùì\n","\n","- A classification problem\n","- The circle dataset (<span style=\"color:blue\">blue dots</span> inside a circle of <span style=\"color:orange\">orange dots</span>)\n","- Ratio of training to test data : $ 70 \\% $\n","- No noise ($ = 0$)\n","- Do not show test data (right panel)\n","- Do not discretize the output\n","- Activation function: ***ReLU***\n","\n","<details>\n","    <summary><i> Why Relu? </i></summary>\n","        \n","üí° In general, try it by default. It appears to work better for many problems!\n","    \n","_Note: Playground only allows you to select **one** activation function that is used for **all** of the **hidden** layers_\n","\n","</details>"]},{"cell_type":"markdown","metadata":{"id":"Plig9ftkB03W"},"source":["### (2) The features"]},{"cell_type":"markdown","metadata":{"id":"PcssBB-iB03W"},"source":["‚ùì <u>Questions about the features</u> ‚ùì\n","\n","1. Select only the features $X_1$ and $X_2$ (_unselect the other features if necessary_)\n","2. If you were using the other variables such as $X_1^{2}$, $X_2^{2}$, $X_1 X_2$, $sin(X_1)$ and $sin(X_2)$, what type of classic Machine Learning operation does it correspond to?"]},{"cell_type":"markdown","metadata":{"tags":["challengify"],"id":"OMS89jAOB03W"},"source":["> Feature engineering"]},{"cell_type":"markdown","metadata":{"id":"hiYCGc37B03W"},"source":["<details>\n","    <summary><i>Answer</i></summary>\n","\n","* It corresponds to some type of ***feature engineering*** where you transform them.\n","    * <i>Examples: multiplication, sinus, square, ...</i>\n","* Here, in this exercise but also tomorrow, we will only use the raw input features $X_1$ and $X_2$.\n","</details>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"NqNyJdnBB03W"},"source":["### (3) Building and Fitting a Neural Network in ***Playground***"]},{"cell_type":"markdown","metadata":{"id":"blt1u6f-B03X"},"source":["‚ùì <u>Questions about Neural Networks</u> ‚ùì\n","\n","* üß† Build a model with the following architecture:\n","    - three hidden layers\n","    - 5 neurons on the first hidden layer\n","    - 4 neurons on the second hidden layer\n","    - 3 neurons on the last hidden layer\n","    - In ***Playground***, the output layer is not represented:\n","        - For such binary classification task, keep in mind that it will automatically be a dense layer with 1 neuron activated by the sigmoid function $ \\large \\phi(z) = \\frac{1}{1 + e^{-z}} $\n","\n","* üí™ ***Fit it and stop the iterations when the loss function has stabilized.***\n","\n","* üëÄ Observe carefully:\n","    - Look at the individual neurons and try to understand what each neuron has specialized for during the _.fit()_\n","    - What do you think about the overall shape your results? Re-run the neural network with different activation functions to compare. Can you make it work with \"Linear\"?"]},{"cell_type":"markdown","metadata":{"id":"1HE700W3B03X"},"source":["<details>\n","    <summary>Answer: some insights about the activation functions</summary>\n","\n","- Results may look like a hexagon because ReLu is piece-wise linear!\n","- A non-linearly separable problem cannot be fitted with a linear activation such as **Linear**\n","- Surprisingly, a piece-wise linear activation function such as **ReLu** (or **LeakyReLu**) fits this non-linearly separable problem well (even if that is not always true)\n","- The `tanh` activation gives a \"smoother\" decision boundary\n","- The **sigmoid** does **not** seem to work well here.\n","    \n","üßëüèª‚Äçüè´ Always start with ReLu, it's a safe bet üßëüèª‚Äçüè´!\n","</details>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"g8WTRNbiB03X"},"source":["### (4) Building and Fitting a Neural Network in ***Tensorflow.Keras***"]},{"cell_type":"markdown","metadata":{"id":"NGPxpmrQB03X"},"source":["üëá We wrote the same model for you - at least the architecture - in Tensorflow's Keras"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1De6KYqjB03X","executionInfo":{"status":"ok","timestamp":1699868040126,"user_tz":-60,"elapsed":8037,"user":{"displayName":"Tanushri Nayak","userId":"14943085948372267228"}},"outputId":"af0551d7-f133-47bf-b78f-90dfee0b84c3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.14.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n","Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.34.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.59.2)\n","Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.1)\n","Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.0)\n","Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.3)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.17.3)\n","Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (1.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.5.1)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.31.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.0.1)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (5.3.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2023.7.22)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow) (2.1.3)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.5.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (3.2.2)\n"]}],"source":["!pip install tensorflow\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"1_dCgAAAB03Z","executionInfo":{"status":"ok","timestamp":1699868050826,"user_tz":-60,"elapsed":6018,"user":{"displayName":"Tanushri Nayak","userId":"14943085948372267228"}}},"outputs":[],"source":["from tensorflow.keras import models\n","from tensorflow.keras import layers\n","\n","model = models.Sequential()\n","\n","model.add(layers.Dense(5, activation='relu', input_dim=2)) # 1st hidden layer with 5 neurons\n","model.add(layers.Dense(4, activation='relu')) # 2nd hidden layer with 4 neurons\n","model.add(layers.Dense(3, activation='relu')) # 3rd hidden layer with 3 neurons\n","\n","\n","\n","model.add(layers.Dense(1, activation='sigmoid')) # Output layer that outputs a probability of belonging\n","                                                 # to the class of \"success\"\n"]},{"cell_type":"markdown","metadata":{"id":"fKkVPmCMB03Z"},"source":["<details>\n","    <summary><i>What to understand about the code of a Neural Network? </i>üëÜ</summary>\n","\n","- <u>First Hidden Layer a.k.a ***Input Layer***</u>:\n","    - Every datapoint that will be input to the neural network has two features $ X = \\begin{bmatrix}\n","           X_{1} \\\\\n","           X_{2} \\\\\n","         \\end{bmatrix} $.\n","    - You need to inform your Neural Network about the ***number of input features*** through the ***`input_dim` argument***\n","    - A Neural Network tries to mimic the human brain. Here we would like to use 5 neurons to start analyzing each of these points.\n","    \n","    - Every datapoint goes through the first hidden layer which was built using 5 neurons $ layer_1 = \\begin{bmatrix}\n","           a_{1} \\\\\n","           a_{2} \\\\\n","           a_{3} \\\\\n","           a_{4} \\\\\n","           a_{5} \\\\           \n","         \\end{bmatrix} $\n","    \n"," - <u>Second Hidden Layer</u>:\n","         \n","    - What if we want to ***make the information flow*** through a second hidden layer with 4 neurons? It is totally possible!\n","    - These 4 neurons $ layer_2 = \\begin{bmatrix}\n","           b_{1} \\\\\n","           b_{2} \\\\\n","           b_{3} \\\\\n","           b_{4} \\\\\n","         \\end{bmatrix} $ from the second layer will analyze the output from the 5 neurons in the first layer\n","    \n","- <u>Third Hidden Layer</u>:\n","        - What if we want the information to **continue to flow** through a third hidden layer with 3 neurons? Again, totally possible!\n","\n","    - Every neuron's output from the second layer goes through the third hidden layer which was built using 3 neurons $ layer_3 = \\begin{bmatrix}\n","           c_{1} \\\\\n","           c_{2} \\\\\n","           c_{3}\n","         \\end{bmatrix} $\n","         \n","    - These 3 neurons analyze the outputs of the neurons in $ layer_2  $ !\n","\n","- <u>Predictive Layer</u>\n","    - You are dealing with a binary classification task\n","    - We could use two neurons to predict the probability of belonging to class A or class B...\n","    - But one neuron predicting the probability of \"success\" is enough\n","\n","- <u>About activation functions</u>\n","    - Despite its simplicity, the ***ReLU*** has proven to be very effective to add some non-linearity to the layers\n","    - For the predictive layer, the best activation function to use for a classification task is the ***sigmoid*** function. That is something we've already discussed during Decision Science and Machine Learning.\n","\n","- <u>About the Sequential aspect of the Network</u>:\n","    - The fact that you are defining a **Sequential** model has a consequence: each layer is aware of its input size based on the output size of the previous layer!\n","    \n","</details>"]},{"cell_type":"markdown","metadata":{"id":"8tdrFBx5B03Z"},"source":["‚ùì How many parameters are involved in this small Neural Network ‚ùì"]},{"cell_type":"code","execution_count":4,"metadata":{"tags":["challengify"],"colab":{"base_uri":"https://localhost:8080/"},"id":"wJ1FUAEWB03a","executionInfo":{"status":"ok","timestamp":1699868277090,"user_tz":-60,"elapsed":230,"user":{"displayName":"Tanushri Nayak","userId":"14943085948372267228"}},"outputId":"f12a30c7-d701-417d-c717-072c0f1832a0"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["58"]},"metadata":{},"execution_count":4}],"source":["# YOUR CODE HERE\n","2*5 + 5 + 5*4 + 4 + 4*3 + 3 + 3*1 + 1"]},{"cell_type":"markdown","metadata":{"id":"yqdL-VFlB03a"},"source":["<details>\n","    <summary><i>Hint</i></summary>\n","\n","‚úÖ You should have 58 parameters\n","    \n","‚ùå If not, double-check your architecture    \n","</details>"]},{"cell_type":"markdown","metadata":{"id":"Na9fbQMVB03a"},"source":["### (5) The XOR Dataset"]},{"cell_type":"markdown","metadata":{"id":"HQnptDLXB03a"},"source":["‚ùì <u>Playing with the XOR Dataset</u> ‚ùì\n","\n","* On Playground:\n","    - Change the dataset to the \"XOR - Exclusive Or\".\n","    - Try to design a model with two hidden layers that has a very small **test loss**\n","        - Note: you are free to choose the number of neurons per layer yourself.  \n","        \n","* Coding with Tensorflow/Keras:\n","    - Once you have built your model on Playground, code it down below with the Tensorflow/Keras library"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tueTLuDjB03a","executionInfo":{"status":"ok","timestamp":1699871055120,"user_tz":-60,"elapsed":11623,"user":{"displayName":"Tanushri Nayak","userId":"14943085948372267228"}},"outputId":"9e579489-60e2-4b4e-f985-cacfb870bb5c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_6\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_12 (Dense)            (None, 20)                60        \n","                                                                 \n"," dense_13 (Dense)            (None, 10)                210       \n","                                                                 \n"," dense_14 (Dense)            (None, 1)                 11        \n","                                                                 \n","=================================================================\n","Total params: 281 (1.10 KB)\n","Trainable params: 281 (1.10 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x7eb3dd7fff10>"]},"metadata":{},"execution_count":10}],"source":["# Neural Network that can be well fitted to the XOR Dataset\n","from tensorflow.keras import Sequential\n","from tensorflow.keras.layers import Dense\n","#from tensorflow.keras.optimizers import Adam\n","\n","model1 = Sequential()\n","\n","model1 = Sequential()\n","model1.add(layers.Dense(20, activation='relu', input_dim=2))\n","model1.add(layers.Dense(10, activation='relu'))\n","model1.add(layers.Dense(1, activation='sigmoid'))\n","model.summary()\n","\n","model1.compile(loss='binary_crossentropy', optimizer='adam',\n","               metrics=['accuracy'])\n","\n","# XOR dataset\n","\n","import numpy as np\n","\n","X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n","y = np.array([0, 1, 1, 0])\n","\n","model1.fit(X, y, epochs=1000, verbose=0)\n"]},{"cell_type":"code","source":["model1.evaluate(X, y)\n","# returns [loss, metrics]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mGdhH2zYNjK8","executionInfo":{"status":"ok","timestamp":1699871120990,"user_tz":-60,"elapsed":605,"user":{"displayName":"Tanushri Nayak","userId":"14943085948372267228"}},"outputId":"cbce3e3e-536a-41aa-9d5e-4b4cd2aff37c"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 150ms/step - loss: 0.0112 - accuracy: 1.0000\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.011227978393435478, 1.0]"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["# 2 hidden layers that has very small test loss and test accuracy\n","\n","# Build the model\n","model2 = models.Sequential()\n","\n","# Add the 2 hidden layers\n","model2.add(layers.Dense(20, activation='relu', input_dim=2))\n","model2.add(layers.Dense(10, activation='relu'))\n","\n","# Add the output layer\n","model2.add(layers.Dense(1, activation='sigmoid'))\n","model2.summary()\n","\n","# Compile the model\n","model2.compile(optimizer='adam',\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","\n","\n","# XOR Dataset\n","X = [[0, 0], [0, 1], [1, 0], [1, 1]]\n","y = [0, 1, 1, 0]\n","\n","# Split the dataset into training and test sets\n","X_train = X[:3]\n","y_train = y[:3]\n","X_test = X[3:]\n","y_test = y[3:]\n","\n","\n","# Train the model\n","model2.fit(X_train, y_train, epochs=1000, batch_size=1, verbose=0)\n","\n","# Evaluate the model\n","test_loss, test_acc = model2.evaluate(X_test, y_test)\n","\n","print('Test accuracy:', test_acc)\n","print('Test loss:', test_loss)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CqYEBBjVMPK1","executionInfo":{"status":"ok","timestamp":1699871438280,"user_tz":-60,"elapsed":54334,"user":{"displayName":"Tanushri Nayak","userId":"14943085948372267228"}},"outputId":"d3d2e5e2-62cd-4ee2-dcc9-0fed2dbd1037"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_10\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_21 (Dense)            (None, 20)                60        \n","                                                                 \n"," dense_22 (Dense)            (None, 10)                210       \n","                                                                 \n"," dense_23 (Dense)            (None, 1)                 11        \n","                                                                 \n","=================================================================\n","Total params: 281 (1.10 KB)\n","Trainable params: 281 (1.10 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","1/1 [==============================] - 0s 163ms/step - loss: 14.2557 - accuracy: 0.0000e+00\n","Test accuracy: 0.0\n","Test loss: 14.255678176879883\n"]}]},{"cell_type":"markdown","metadata":{"id":"Ob7Ojh6pB03a"},"source":["### (6) The Spiral Dataset"]},{"cell_type":"markdown","metadata":{"id":"8z-gydNnB03a"},"source":["‚ùì <u>Playing with the Spiral Dataset</u> ‚ùì\n","\n","* On Playground:\n","    - Change the dataset to the \"Spiral\".\n","    - Try to design a model with three hidden layers that has a very small **test loss**\n","        - Note: you are free to choose the number of neurons per layer yourself.  \n","        \n","* Coding with Tensorflow/Keras:\n","    - Once you have built your model on Playground, code it down  below with the Tensorflow/Keras library"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":429},"id":"SLkWh9pYB03b","executionInfo":{"status":"error","timestamp":1699871856083,"user_tz":-60,"elapsed":2383,"user":{"displayName":"Tanushri Nayak","userId":"14943085948372267228"}},"outputId":"00c9220e-a0d0-4097-f234-2740928c9ae3"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-43e5e08e8bec>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Spiral dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spiral.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spiral_labels.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'spiral.npy'"]}],"source":["# Neural Network that can be well fitted to the Spiral Dataset\n","\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","\n","# Spiral dataset\n","X = np.load('spiral.npy')\n","y = np.load('spiral_labels.npy')\n","\n","# Train test split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,\n","                                                    random_state=3)\n","# Standardize\n","from sklearn.preprocessing import StandardScaler\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","\n","# Build the model\n","model = models.Sequential()\n","\n","# 3 hidden layers\n","model.add(layers.Dense(5, activation='relu', input_dim=2))\n","model.add(layers.Dense(4, activation='relu'))\n","model.add(layers.Dense(3, activation='relu'))\n","\n","# Output layer\n","model.add(layers.Dense(3, activation='softmax'))\n","\n","model.summary()\n","\n","model.compile(\n","    optimizer='adam',\n","    loss='binary_crossentropy',\n","    metrics = 'accuracy')\n","\n","model.fit(X_train, y_train, batch_size=16, epochs=20)\n","\n","\n","model.evaluate(scaler.transform(X_test), y_test)\n","# returns [loss, metrics]\n"]},{"cell_type":"markdown","metadata":{"id":"wK2ruvD9B03b"},"source":["### (7) How Deep should a Neural Network be ?"]},{"cell_type":"markdown","metadata":{"id":"C92STkjkB03b"},"source":["üëÄ If you compare the number of parameters needed to fit the Spiral Dataset vs. the XOR dataset, the former requires many more weights....\n","\n","üòÉ Actually, if your models are deep enough, you could potentially fit pretty much any pattern...\n","\n","---\n","\n","<details>\n","    <summary><i>Should I create Very Deep Neural Networks? </i></summary>\n","        \n","<u>Examples:</u>\n","    \n","* Think about a human being. The more this person spends time coding in Python, the better he/she will get better at it!\n","    \n","* Think about a student. The more this person studies, the better he/she will pass exams. But sometimes students can study \"too much\" about a topic and forget about the global picture of a course....\n","    \n","<u>Lessons</u>\n","    \n","üß† For Deep Learning Models, the more layers they have, the more opportunities they will have to learn the patterns in the data...\n","\n","‚ùóÔ∏è The problem is about avoiding **overfitting** ‚ùóÔ∏è\n","    \n","‚ò†Ô∏è Add a good deal of noise and you _may_ see that your model will have learned \"too much\" about this noise.\n","  \n","    \n","üìÜ The next lecture **Deep Learning > Optimizers, Loss, & Fitting** is dedicated to helping you understand which techniques we can use to prevent Deep Learning models from overfitting.\n","\n","</details>\n","\n","---\n","\n","<details>\n","    <summary><i>A picture of overfitting in Playground</i></summary>\n","    \n","<img src='https://wagon-public-datasets.s3.amazonaws.com/data-science-images/DL/playground-overfitting.png' width=700 style='margin:auto'>\n","</details>\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"W4gfKulgB03b"},"source":["## Regression in Deep Learning"]},{"cell_type":"markdown","metadata":{"id":"P9AyToYWB03b"},"source":["<u>Let's try to complete a Regression Task using Deep Learning</u>\n","\n","\n","This time, the last layer will no longer look like:  \n","```python\n","model.add(layers.Dense(1, activation='sigmoid'))\n","```\n","\n","but instead  :\n","```python\n","model.add(layers.Dense(1, activation='linear'))\n","```\n","\n","This means that the output of this network is no longer between $0$ and $1$ (probability) but between $ -\\infty$ and $+ \\infty$."]},{"cell_type":"markdown","metadata":{"id":"pDOPmz_BB03c"},"source":["‚ùì <u>Playing with the Regression Dataset</u> ‚ùì\n","\n","* On Playground:\n","    - Change the dataset to the \"Regression\".\n","    - Try to design a model that has a very small **test loss**\n","        - Note: you are free to choose both the number of layers and the number of neurons per layer yourself\n","        \n","* Coding with Tensorflow/Keras:\n","    - Once you have built your model on Playground, code it down  below with the Tensorflow/Keras library"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t9w5xSg9B03c"},"outputs":[],"source":["# Neural Network that can be well fitted to the Regression Dataset\n","\n","pass  # YOUR CODE HERE\n"]},{"cell_type":"markdown","metadata":{"id":"ROG2e9pmB03c"},"source":["üèÅ You are now ready to do the same things with Tensorflow's Keras directly!\n","\n","üí™ This was a Warm-Up about Neural Networks / Deep Learning Models... (even if, admittedly, our networks in this challenge were not so \"deep\").\n","\n","\n","üíæ Don't forget to `git add/commit/push` your notebook...\n","\n","üöÄ ... and move on to the next challenge!"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}